{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question I Build the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "import numpy as np\n",
    "\n",
    "#Load the data and training\n",
    "sky_data = np.loadtxt('skysurvey/training_data.csv', delimiter=\",\")\n",
    "sky_class = np.loadtxt('skysurvey/training_class.csv')\n",
    "clf = tree.DecisionTreeClassifier(min_samples_leaf=0.01)\n",
    "clf.fit(sky_data, sky_class)\n",
    "\n",
    "#Draw the tree into a pdf\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data, filled= True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"Tree.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the tree we get is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tree](img\\Tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question II The generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition :\n",
    "> Gener. error = training error + 0.5 x #of leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the code of calculate the generalization error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 1.13%\n",
      "Number of nodes in the tree: 55\n",
      "Generalization error: 1.27%\n"
     ]
    }
   ],
   "source": [
    "#Calculate the generalization error\n",
    "training_error = (len(sky_class) - clf.score(sky_data,sky_class)*len(sky_class))/len(sky_class)\n",
    "array_children = clf.tree_.children_left\n",
    "num_leaves = np.count_nonzero(array_children == -1)\n",
    "gen_error = training_error + (0.5*num_leaves/len(sky_class))\n",
    "print(\"Training error: \"+str(100*training_error)+\"%\")\n",
    "print(\"Number of nodes in the tree: \" +str(clf.tree_.node_count))\n",
    "print('Generalization error: ' + str(100*gen_error) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question III Change the parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the results of the two precedent questions, the training error we got is 1.13%, which is near to the *minimun sample leafs* proportion we have chosen. If we observe the tree obtained in question 1, we can see that after the second layer, because the exceptions in each branch are less than 1% of the total instances and are distributed, which means they are not sufficient to become a leaf, so that they are not successfully extracted.\n",
    "\n",
    "So, we cannot reduce the training error by pre-pruning the data with the parameters of this classifer, what we can do in order to minimize the number of leaves is to cut as many leaves as possible without changing the classification result. If we observe the tree generated in the first question, we can see that we can cut all the nodes in the left branch up to the first layer, and the right branch up to the second layer.\n",
    "\n",
    "For achieving this, we can simply set ***max_leaf_node = 3***, this will minimize the leafs cause we need at least three nodes for three possible class and in the tree of question 1, no new significant division is generated from the first three nodes. \n",
    "\n",
    "the code, the result is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 1.13%\n",
      "Number of leaves: 3\n",
      "Generalization error: 1.145%\n"
     ]
    }
   ],
   "source": [
    "#Load the data and training\n",
    "clf = tree.DecisionTreeClassifier(min_samples_leaf=0.01, max_leaf_nodes = 3)\n",
    "clf.fit(sky_data, sky_class)\n",
    "\n",
    "#Draw the tree into a pdf\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data, filled=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"Tree_modified.pdf\")\n",
    "\n",
    "#Calculate the generalization error\n",
    "training_error = (len(sky_class) - clf.score(sky_data,sky_class)*len(sky_class))/len(sky_class)\n",
    "array_children = clf.tree_.children_left\n",
    "num_leaves = np.count_nonzero(array_children == -1)\n",
    "gen_error = training_error + (0.5*num_leaves/len(sky_class))\n",
    "print('Training error: ' + str(100*training_error) + '%')\n",
    "print('Number of leaves: ' + str(num_leaves))\n",
    "print('Generalization error: ' + str(100*gen_error) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training error has not changed even if we has set the number of leaves to 3, the minimum value possible for this case. Yet another way to has a tree like this is to set ***min_impurity_decrease=t***, where ***t*** is a value big enough to prevent the second spilt in the left main branch but no too big to cease the spilt in the right branch. Amd we can also set *t* in a value that can exactly give us the three leaves tree we need, this is possible because except the three splits , all other spilt will just decrease the impurity a very little bit, any value between ***0.01(1%, the minimum sample leafs)*** and ***0.113(1.13%, the training error)*** will do (**En effet, the real range will be wider than this, but the calculation of the exact range is pointless**).\n",
    "\n",
    "Or, more simply, we can add *max_depth=2*, and use a very samll value of *t*, because the split in the leaf has just decreased the impurity for a negligible amount. For instance, ***max_depth=2, min_impurity_decrease=0.00003*** will do.\n",
    "\n",
    "Below is the tree obtained in these three ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tree Modified](img\\Tree_modified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question IV Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is direct, I will choose **the best one I obtained in point 2**, If the final classification result is the same, we do not need to perform the following classification procedures, the latter tree will deliver the same result, but just in two comparisons in maximum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question V Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "three_object = np.array([[199, 0.2, 19, 18, 18, 18, 16, 777, 301, 3.0, 270, 3.312, 0.001, 288, 51739, 550],\n",
    "                       [199, 0.2, 19, 18, 18, 18, 16, 777, 301, 3.0, 270, 3.312, 0.119, 288, 51739, 550],\n",
    "                       [199, 0.2, 19, 18, 18, 18, 16, 777, 301, 3.0, 270, 3.312, 0.219, 288, 51739, 550]])\n",
    "Class_value = clf.predict(three_object)\n",
    "print(Class_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test, I have constructed three objects which carry the exact same data except the differences in **X\\[12\\]**, which is the **\"Final Redshift\"** of the sky object, but we have seen that the three object are successsfully classified.\n",
    "\n",
    "The **insights** for this tree need to be discussed in two aspects. **On one hand**, if a 1.145% error rate is acceptable, we can conclude that only the redshift feature is enough to distinguish stars, galaxies and Quasars so that we can use it as a criterion, or at least we can say that the redshift is the main difference between these three categories. **On the other hand**, a 1.145% error rate is still a little high, and this tree is even kind of robust because all the other features haven't been used, we can still get the result from the redshift no matter how ridiculous the other features are. But we have to admit that by *pruning* like this, we do have minimized the generalization error.\n",
    "\n",
    "Consequently, it will be kind of a dilemma if we can neither accept an error over 1% nor simply take redshift as a criterion. However in fact an error that is just a little larger than 1% is quite normal with the minimum sample numbers for a leaf being 1% of total instances, this tree don't have problems like *underfitting* or *overfitting* to this extent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question VI Post-pruning or not ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is no. Now I only have three leaves and three classes to classify in the meantime, thus it is not possible to perform further pruning. Any further pruning will cause a whole class of instances to be misplaced, which will greatly increase the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question VII The implement of post-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented the post-pruning policy like this. But on top of what we have discussed in the precedent questions, I choose to **add an Exception** to throw when the tree got in can no longer be pruned. When the tree can be pruned, in this case, I have firstly caculated the misplaced instances in a node, if the number of misplaced instances in this node is smaller than the minimum requirement for a leaf, we can cut it because it will either not increase the training error, or increase the error by a amount less than 0.5 x \\#leaves_cutted, which will in turn **reduce the generalization error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Tree can no longer be pruned.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-576f7ac0de04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren_right\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mpost_pruning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-576f7ac0de04>\u001b[0m in \u001b[0;36mpost_pruning\u001b[1;34m(tree)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tree can no longer be pruned.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Tree can no longer be pruned."
     ]
    }
   ],
   "source": [
    "def post_pruning(tree):\n",
    "    min_leaf = 1\n",
    "    tempm = tree.min_samples_leaf\n",
    "    if tempm != None:\n",
    "        if tempm < 1:\n",
    "            min_leaf = tempm * clf.tree_.n_node_samples[0]\n",
    "        if tempm > 1:\n",
    "            min_leaf = tempm    \n",
    "    array_children = tree.tree_.children_left\n",
    "    num_leaves = np.count_nonzero(array_children == -1)\n",
    "    n = tree.n_classes_\n",
    "    if n >= num_leaves:\n",
    "        raise Exception('Tree can no longer be pruned.')\n",
    "    else:\n",
    "        for i in range(tree.tree_.node_count):\n",
    "            values = tree.tree_.value[i,0]\n",
    "            instance_misplaced = sum(values)-max(values)\n",
    "            if instance_misplaced < min_leaf:\n",
    "                tree.tree_.children_left[i] = -1\n",
    "                tree.tree_.children_right[i] = -1\n",
    "\n",
    "post_pruning(clf)\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data, filled= True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"Tree_pruned.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above we can see that, this funtion returned the exception as expected for the **best tree in point 2**, But if we test this function with the tree we got from the default configuration, alias **the tree in point 1**, we can get a tree same as the best tree in point 2, which means this function works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
