{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA211 TP1 Nonnegative Matrix Factorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import the dataset to the Matrix to verify the number of images and number of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images is: 400\n",
      "The number of pixels in an image is: 10304\n"
     ]
    }
   ],
   "source": [
    "#(Ignore this if you have installed Anaconda) LaTeX packages and Python 3 are needed for the display of formules.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "def build_matrix_from_faces(folder='att_faces', minidata=False):\n",
    "    # load images\n",
    "    # 400 images of size (112, 92)\n",
    "    M = []\n",
    "    if minidata is True:\n",
    "        nb_subjects = 1\n",
    "    else:\n",
    "        nb_subjects = 40\n",
    "    for subject in range(1, nb_subjects + 1\n",
    "                        ):\n",
    "        for image in range(1, 11):\n",
    "            face = plt.imread(folder + '/s' + str(subject)\n",
    "                              + '/' + str(image) + '.pgm')\n",
    "            M.append(face.ravel())\n",
    "\n",
    "    return np.array(M, dtype=float)\n",
    "\n",
    "def vectorize(W, H):\n",
    "    return np.concatenate((W.ravel(), H.ravel()))\n",
    "\n",
    "def unvectorize_M(W_H, M):\n",
    "    # number of elements in W_H is (n+p)*k where M is of size n x m\n",
    "    # W has the nk first elements\n",
    "    # H has the kp last elements\n",
    "    n, p = M.shape\n",
    "    k = W_H.shape[0] // (n + p)\n",
    "    W = W_H[:n * k].reshape((n, k))\n",
    "    H = W_H[n * k:].reshape((k, p))\n",
    "    return W, H\n",
    "\n",
    "M = build_matrix_from_faces(folder='att_faces', minidata=False)\n",
    "def unvectorize(W_H): return unvectorize_M(W_H, M)\n",
    "k = 38\n",
    "\n",
    "n = M.shape[0]\n",
    "p = M.shape[1]\n",
    "print(\"The number of images is: \" + str(n))\n",
    "print(\"The number of pixels in an image is: \" + str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are $400$ images in the database, and $10304$ pixels in each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Presentation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the scalar case with $n=p=1$, Then the function is:\n",
    "$$arg \\min_{W\\geq 0, H \\geq 0}(m-wh)^{2}$$\n",
    "We can get the Hessian matrix\n",
    "$$ \\nabla^{2}f(w,h) = \\begin{bmatrix} 2h^{2}\n",
    " & 4hw-2m \\\\ 4wh-2m\n",
    " & 2w^{2}\n",
    "\\end{bmatrix}$$\n",
    "The Hessian matrix is not positive semidefinite for all values. For example, \n",
    "$$\\nabla^{2}f(2,1)=\\begin{bmatrix} 2\n",
    " & 6 \\\\ 6\n",
    " & 8\n",
    "\\end{bmatrix}, \\lambda_{min}=-1.7082 < 0$$\n",
    "So the objective function is not convex. <br><br>\n",
    "Then, to calculate the gradient, we note the objective function as $$f(W,H)=\\frac{1}{2np} \\sum^{n}_{i=1} \\sum^{p}_{l=1}(M_{i,l}-\\sum^{k}_{j=1}W_{i,j}H_{j,l})^{2}$$\n",
    "We can observe that $\\sum^{k}_{j=1}W_{i,j}H_{j,l} = (WH)_{i,l}$, so we continue with this notation.\n",
    "For one of the element in location $a,b$ of matrix $\\nabla_{W}f(W,H)$,  we have\n",
    "$$\\frac{\\partial f(W,H)}{\\partial w_{a,b}}=\\frac{1}{np} \\sum_{l=1}^{p}[M_{a,l}-(WH)_{a,l}]\\cdot -H_{b,l}=\\frac{1}{np} \\sum_{l=1}^{p}(WH-M)_{a,l}H_{b,l}=[\\frac{1}{np}(WH-M)H^{T}]_{a,b}$$\n",
    "Similarly, we have\n",
    "$$\\frac{\\partial f(W,H)}{\\partial h_{a,b}}=[\\frac{1}{np}W^{T}(WH-M)]_{a,b}$$\n",
    "So, The gradient of the function $f(W,H)$ consists of two parts:\n",
    "$$\\nabla_{W}f(W,H)=\\frac{1}{np}(WH-M)H^{T}$$\n",
    "$$\\nabla_{H}f(W,H)=\\frac{1}{np}W^{T}(WH-M)$$\n",
    "From above, we can conclude that its gradient is not Lipschitz continuous because the degree of the gradient is 2, which is greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Find $W$ when $H_{0}$ is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here regenerates a small data set to test algorithm for the following questions before contruct $W_{0}$ and $H_{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small data to test the algorithm\n",
    "M = build_matrix_from_faces(folder='att_faces', minidata=True)\n",
    "def unvectorize(W_H): return unvectorize_M(W_H, M)\n",
    "k = 2\n",
    "n = M.shape[0]\n",
    "p = M.shape[1]\n",
    "\n",
    "W0, S, H0 = scipy.sparse.linalg.svds(M, k)\n",
    "W0 = np.maximum(0, W0 * np.sqrt(S))\n",
    "H0 = np.maximum(0,(H0.T * np.sqrt(S)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decompostion is precise with $M=W_{0}SH_{0}$, and the $S$ ensures the non-negativity of $M_{0}$ and $S_{0}$, I feel also that these two matrices is close to the minimum we want to find, this will save a lot of computation time.\n",
    "Other possibilities we can consider are:\n",
    "1. Use $W_{0}=0$ and $H_{0}=0$\n",
    "2. Use the identity matrix\n",
    "3. Initialize each rows of the $W_{0}$ with the average of $n$ random rows from $M$, similar for each column of $H_{0}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with $H_{0}$ fixed, we can find than the objective function is a composition of a linear function $u(W) = M - WH_{0}$ ans two convex functions $v(A)=||A||_{F}$ and $w(x)=x^{2}$, which can be shown as $$g(W) = \\frac{1}{2np} * w \\circ v \\circ u(W)$$\n",
    "So, as those operations all preserve the convexity, $g(W)$ is convex.<br>\n",
    "The gradient of $g(W)$ is\n",
    "$$\\nabla_{W}g(W)=\\frac{1}{np}(WH_{0}-M)H_{0}^{T}$$ by simply replace $H$ with $H_{0}$ from the question above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions are written with following functions that we have obtained earlier.\n",
    "$$g(W)=\\frac{1}{2np}||M-WH_{0}||^{2}_{F}$$\n",
    "$$\\nabla_{W}g(W)=\\frac{1}{np}(WH_{0}-M)H_{0}^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value computed is: 456.31969468823456\n",
      "The gradient is: [[ 5.65746427e-01  2.78479793e-15]\n",
      " [ 2.19176669e-01 -3.53296412e-14]\n",
      " [ 6.24393539e-01 -2.82120773e-14]\n",
      " [-8.03700500e-02  1.11216656e+00]\n",
      " [-3.78297174e-02  7.25830512e-01]\n",
      " [ 8.56859007e-02  1.39195763e-14]\n",
      " [ 1.55403714e-01 -6.00210013e-15]\n",
      " [ 1.70087529e-01  1.15419963e+00]\n",
      " [-5.69108263e-02  1.81638808e+00]\n",
      " [ 9.70044928e-02  5.76551946e-01]]\n",
      "The check result is: 2.3372820015034315e-05\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#The function to compute g(W)\n",
    "def val_g(W):\n",
    "    W = W.reshape(n, k)\n",
    "    g_W = norm(M - W.dot(H0), ord = 'fro')**2/(2 * n * p)\n",
    "    return g_W\n",
    "\n",
    "#The function to compute the deriavative\n",
    "def grad_g(W):\n",
    "    W = W.reshape(n, k)\n",
    "    g_w_grad = (W.dot(H0)-M).dot(H0.T)/(n * p)\n",
    "    return g_w_grad\n",
    "\n",
    "#This function is just for verification with a raveled return value.\n",
    "def grad_g_for_check(W):\n",
    "    W = W.reshape(n, k)\n",
    "    g_w_grad = (W.dot(H0)-M).dot(H0.T)/(n * p)\n",
    "    return g_w_grad.ravel()\n",
    "\n",
    "print(\"The value computed is: \" + str(val_g(W0)))\n",
    "print(\"The gradient is: \" + str(grad_g(W0)))\n",
    "print(\"The check result is: \" + str(check_grad(val_g, grad_g_for_check, W0.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error checked is very small, the computation is right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of **3.2 Proximal point method** of the course, we know that \n",
    "$$prox_{\\gamma\\iota_{\\mathbb{R}_{+}}}(x)=\\mathop{\\arg\\min}_{y\\in\\mathbb{R}}(\\gamma g(y)+\\frac{1}{2}\\lVert y-x\\rVert^2) =\\mathop{\\arg\\min}_{y\\in\\mathbb{R}}(\\gamma\\iota_{\\mathbb{R}_{+}}(y)+\\frac{1}{2}\\lVert y-x\\rVert^2)$$\n",
    "In order to find the proximal point(s), we take the deriavative of the function by $y$, which is\n",
    "$$\\frac{d prox_{\\gamma\\iota_{\\mathbb{R}_{+}}}}{dy} = \\left\\{\\begin{aligned}\n",
    "y-x & & y>0 \\\\\n",
    "\\infty & & y\\leq0 \n",
    "\\end{aligned}\\right.\n",
    "$$\n",
    "We can see that, for the derivative, we can only obtain $0$ in thie point $y - x = 0$, which means $prox_{\\gamma\\iota_{\\mathbb{R}_{+}}}(x)=y=x>0$, the projection stands. <br>\n",
    "So, $prox_{\\gamma\\iota_{\\mathbb{R}_{+}}}$ is the projection onto $\\mathbb{R}_{+}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function here is $$W_{t+1} = prox_{\\gamma g}(W_{t} - \\gamma \\nabla g(W_{t}))= max(0, W_{t} - \\gamma \\nabla g(W_{t}))$$ And we take $$\\gamma = \\frac{1}{L_{0}}=\\frac{np}{||(H_{0})^{T}H_{0}||}$$ So the code below is written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method(val_g, grad_g, W0, gamma, N):\n",
    "    W = W0\n",
    "    for i in range (0, N):\n",
    "        W = W - gamma * grad_g(W)\n",
    "        W = W.clip(min = 0)\n",
    "    return val_g(W), W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimized value is: 442.2878543960478\n"
     ]
    }
   ],
   "source": [
    "L0 = norm(H0.T.dot(H0))/(n*p)\n",
    "val = projected_gradient_method(val_g, grad_g, W0, 1/L0, 100)[0]\n",
    "print(\"The minimized value is: \"+ str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Algorithmic refinement for the problem with $H_{0}$ fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a Taylor-based line search as shown in the **Chapter 3** of the course is implemented: <br>\n",
    "We choose $\\gamma_{k}$ such that for $x^{+}(\\gamma_{k})=prox_{\\gamma_{k}g}(x_{k}-\\gamma_{k}\\nabla f(x_{k}))$, and\n",
    "$$f(x^{+}(\\gamma_{k}))\\leq f(x_{k})+<\\nabla f(x_{k}), x^{+}(\\gamma_{k}-x_{k})>+\\frac{1}{2\\gamma_{k}}||x_{k}-x^{+}(\\gamma_{k})||^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(gamma, f, gradf, w0, a=0.5, b=None):\n",
    "\n",
    "    if b is None:\n",
    "        b = 2 * gamma\n",
    "\n",
    "    assert 0 < a < 1\n",
    "    assert b > 0\n",
    "\n",
    "    l = 0\n",
    "    x = w0\n",
    "\n",
    "    def xplus(gamma, x, gradf):\n",
    "        x = x - gamma * gradf(x)\n",
    "        x = x.clip(min = 0)\n",
    "        return x\n",
    "\n",
    "    fx = f(x)\n",
    "    gradfx = gradf(x)\n",
    "    \n",
    "    while True:\n",
    "        gamma = b * np.power(a, l)\n",
    "        xp = xplus(gamma, x, gradf)\n",
    "        lhs = f(xp)\n",
    "        \n",
    "        rhs = fx + np.vdot(gradfx, xp - x) + \\\n",
    "            np.linalg.norm(x - xp)**2 / (2 * gamma)\n",
    "    \n",
    "        if lhs <= rhs:\n",
    "            break\n",
    "\n",
    "        l += 1\n",
    "\n",
    "    return b * np.power(a, l)\n",
    "\n",
    "def projected_gradient_method_with_linesearch(val_g, grad_g, W0, gamma, N, a = 0.5, b = None):\n",
    "    W = W0\n",
    "    for i in range (0, N):\n",
    "        gamma = linesearch(gamma, val_g, grad_g, W, a, b)\n",
    "        W = W - gamma * grad_g(W)\n",
    "        W = W.clip(min = 0)\n",
    "    return val_g(W), W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly test the time consumption of the algorithm **with line search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with line search is: 442.21792646692586\n",
      "Wall time: 350 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val1 = projected_gradient_method_with_linesearch(val_g, grad_g, W0, 1, 100, 0.5)[0]\n",
    "print(\"The result with line search is: \" + str(val1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the original one **without line search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the method in 3.6 is: 442.2878543960478\n",
      "Wall time: 53.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val = projected_gradient_method(val_g, grad_g, W0, 1/L0, 100)[0]\n",
    "print(\"The result of the method in 3.6 is: \" + str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see two things in the result we get. <br>\n",
    "Firstly, we have a better result from the one with line search. And the reason is that the descending direction we found with line search is an optimal choice for this iteration, but the one using the Lipschitz constant for $\\gamma$ can only guarantee that there will be a descend. So the result with line search is better with the same number of iterations. <br>\n",
    "Secondly, we can see that with the same number of iteration, the one with line search take 5 to 6 times more time than the original one, which is obvious cause it takes time to calculate the $\\gamma_{k}$ in each iteration, and the increasing in time is really significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Resolution of the full problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val2 = projected_gradient_method_with_linesearch(val_g, grad_g, W0, 1, 1000, 0.5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got a result that is really close to the result of 100 iterations, which means this value is not getting smaller, the algorithm returned a (local) minimum for the fixed $H_{0}$ condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the $f(W,H)$ we used in Question 2.1, from the definition of $argmin$, we can know that at iteration, we have\n",
    "$$f(W_{t},H_{t-1})\\leq f(W_{t-1}, H_{t-1})$$\n",
    "$$f(W_{t}, H_{t})\\leq f(W_{t}, H_{t-1})$$\n",
    "So,\n",
    "$$f(W_{t},H_{t})\\leq f(W_{t-1},H_{t-1})$$\n",
    "So that the value of the objective is decreasing in $\\mathbb{R_{+}}$ <br>\n",
    "As we also has the lower bound $0$ for this decreasing problem, we can conclude that the value converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do the alternate method, firstly the functions $$g(H)=\\frac{1}{2np}||M-W_{0}H||^{2}_{F}$$ and\n",
    "$$\\nabla_{H}g(H)=\\frac{1}{np}W_{0}^{T}(W_{0}H-M)$$ are implemented.<br>\n",
    "Then, the functions are used in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_h(H):\n",
    "    H = H.reshape(k, p)\n",
    "    g_H = norm(M - W0.dot(H), ord = 'fro')**2/(2 * n * p)\n",
    "    return g_H\n",
    "    \n",
    "def grad_h(H):\n",
    "    H = H.reshape(k, p)\n",
    "    g_h_grad = (W0.T).dot(W0.dot(H)-M)/(n * p)\n",
    "    return g_h_grad\n",
    "    \n",
    "def alternate_minimization(val_g, grad_g, val_h, grad_h, W0, H0, N, gamma, a = 0.5, b = None):\n",
    "    W = W0\n",
    "    H = H0\n",
    "    for i in range(0, N):\n",
    "        gamma = linesearch(gamma, val_g, grad_g, W, a, b)\n",
    "        W = W - gamma * grad_g(W)\n",
    "        W = W.clip(min = 0)\n",
    "        gamma = linesearch(gamma, val_h, grad_h, H, a, b)\n",
    "        H = H - gamma * grad_h(H)\n",
    "        H = H.clip(min = 0)\n",
    "        \n",
    "    return val_h(H), W, H\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the method in 3.6 is: 442.2878543960478\n",
      "Wall time: 57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val = projected_gradient_method(val_g, grad_g, W0, 1/L0, 100)[0]\n",
    "print(\"The result of the method in 3.6 is: \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result with line search is: 442.21792646692586\n",
      "Wall time: 410 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val1 = projected_gradient_method_with_linesearch(val_g, grad_g, W0, 1, 100, 0.5)[0]\n",
    "print(\"The result with line search is: \" + str(val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of alternate minimization method is: 439.4372725874233\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val3 = alternate_minimization(val_g, grad_g, val_h, grad_h, W0, H0, 100, 1)[0]\n",
    "print(\"The result of alternate minimization method is: \" + str(val3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, with the alternate minimization method and 100 iterations, we get a object value even smaller than the \"local minimum\" we found fixing only $H_{0}$. Clearly, the alternate method takes the most of the time because we do line search two times in each iteration, but it has given a better result because both the values in $W$ and $H$ are optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first thought, I believe that **checking the difference between recent iterations** could be a good stopping criterion. If the difference is small enough, then the procedure stops. However, such a stopping condition does not reveal whether a solution\n",
    "is close to a stationary point or not.<br>\n",
    "Additionally, from the 1.3 part of the course, I think a practicable $\\epsilon -solution$ could be a criterion $T_{\\epsilon}(W,K)$ that\n",
    "$$||prox_{\\gamma }(\\nabla f(W^{t},H^{t}))||_{F} \\leq \\epsilon ||\\nabla f(W^{0}, H^{0})||_{F}$$\n",
    "in which $prox_{\\gamma }$ is the projection we ued in Question 3.4 and 3.5. This way, we stop the algorithm when we don't have a good descend judged by the threshold $\\epsilon$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
